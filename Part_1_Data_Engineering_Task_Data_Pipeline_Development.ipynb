{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1jdeLPHVn47TJOlzT6v2_VUO3hBiQBhmj",
      "authorship_tag": "ABX9TyNzGzPXIoHmrnDP/yfma7i0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/C-Oluwashola/EmployeeChurn-Using-IBM/blob/main/Part_1_Data_Engineering_Task_Data_Pipeline_Development.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A-xCbVB_lRI9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Data Collection\n",
        "def read_data(file_path):\n",
        "    \"\"\"\n",
        "    Read the CSV file into a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        data = pd.read_csv('/content/Tweets.csv')\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading data: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Data Preprocessing\n",
        "def preprocess_data(data):\n",
        "    \"\"\"\n",
        "    Perform basic preprocessing on the data.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Drop irrelevant columns\n",
        "        data = data[['airline_sentiment', 'text']]\n",
        "\n",
        "        # Clean text (remove special characters, URLs, etc.)\n",
        "        data['clean_text'] = data['text'].str.replace(r\"http\\S+|www\\S+|<.*?>\", \"\", regex=True)\n",
        "        data['clean_text'] = data['clean_text'].str.replace(r\"[^a-zA-Z]\", \" \", regex=True)\n",
        "\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error preprocessing data: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "7KKTMp_ToEGj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Data Storage\n",
        "def save_processed_data(data, output_path):\n",
        "    \"\"\"\n",
        "    Save the processed data to a new CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        data.to_csv(output_path, index=False)\n",
        "        print(\"Processed data saved successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving processed data: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define file paths\n",
        "    input_file = \"/content/Tweets.csv\"\n",
        "    output_file = \"processed_tweets.csv\""
      ],
      "metadata": {
        "id": "d_yX2OJ6oQV9"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "tweets_df = pd.read_csv(\"/content/Tweets.csv\")\n",
        "\n",
        "# Clean text (remove special characters, URLs, etc.)\n",
        "tweets_df['clean_text'] = tweets_df['text'].str.replace(r\"http\\S+|www\\S+|<.*?>\", \"\", regex=True)\n",
        "tweets_df['clean_text'] = tweets_df['clean_text'].str.replace(r\"[^a-zA-Z]\", \" \", regex=True)\n",
        "\n",
        "# Save the processed data to a new CSV file\n",
        "tweets_df.to_csv(\"processed_tweets.csv\", index=False)\n",
        "print(\"Processed data saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1czjMr65oxSc",
        "outputId": "a7ee10ab-0f13-4718-a71f-24e676ff19e0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed data saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Design a data pipeline using Python and any appropriate libraries to ingest the simulated dataset into the CDP.\n"
      ],
      "metadata": {
        "id": "sgYaa-f-rODG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the dataset into a pandas DataFrame\n",
        "tweets_df = pd.read_csv(\"/content/tweets_data_ingested.csv\")\n",
        "\n",
        "# Data preprocessing (if needed)\n",
        "# For demonstration purposes, let's assume no preprocessing is required\n",
        "\n",
        "# Save the data to CSV (for demonstration, you can ingest it into the CDP)\n",
        "tweets_df.to_csv(\"tweets_data_ingested.csv\", index=False)\n",
        "\n",
        "# Ingest data into the CDP (simulated)\n",
        "print(\"Tweets data ingestion into CDP completed successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IY6Ht2uBrTtE",
        "outputId": "47db2de2-23a5-451b-b9ba-4bb619e0986f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweets data ingestion into CDP completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Implement error handling mechanisms and ensure data reliability and quality during the ingestion process. (Think of that how would you deal with duplicates or other common data\n",
        "quality issue)#\n"
      ],
      "metadata": {
        "id": "e_2Ax98utuHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the dataset into a pandas DataFrame\n",
        "try:\n",
        "    tweets_df = pd.read_csv(\"/content/tweets_data_ingested.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: File not found.\")\n",
        "    exit()\n",
        "\n",
        "# Handle missing values\n",
        "if tweets_df.isnull().values.any():\n",
        "    print(\"Warning: Missing values found in the dataset. Cleaning missing values...\")\n",
        "    tweets_df.dropna(inplace=True)\n",
        "\n",
        "# Handle duplicates\n",
        "if tweets_df.duplicated().any():\n",
        "    print(\"Warning: Duplicates found in the dataset. Removing duplicates...\")\n",
        "    tweets_df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Data preprocessing (if needed)\n",
        "# For demonstration purposes, let's assume no further preprocessing is required\n",
        "\n",
        "# Save the data to CSV (for demonstration, you can ingest it into the CDP)\n",
        "tweets_df.to_csv(\"tweets_data_ingested.csv\", index=False)"
      ],
      "metadata": {
        "id": "-S2E1lr8txf7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ingest data into the CDP (simulated)\n",
        "print(\"Tweets data ingestion into CDP completed successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bor2YPIEvZHz",
        "outputId": "403d0c9d-e3f0-483b-bde1-ba770484c331"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweets data ingestion into CDP completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 4 Document your approach and any assumptions made during the development process."
      ],
      "metadata": {
        "id": "DjonHDn1wKG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1: Read the Dataset\n",
        "try:\n",
        "    tweets_df = pd.read_csv(\"/content/tweets_data_ingested.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: File not found.\")\n",
        "    exit()"
      ],
      "metadata": {
        "id": "ZYgC9nzCw6U_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2: Handle Missing Values\n",
        "if tweets_df.isnull().values.any():\n",
        "    print(\"Warning: Missing values found in the dataset. Cleaning missing values...\")\n",
        "    tweets_df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "w33GeVeEw_W3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3: Handling Duplicate\n",
        "if tweets_df.duplicated().any():\n",
        "    print(\"Warning: Duplicates found in the dataset. Removing duplicates...\")\n",
        "    tweets_df.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "nlIE-JJOxEeJ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 5: Save the Data\n",
        "tweets_df.to_csv(\"tweets_data_ingested.csv\", index=False)"
      ],
      "metadata": {
        "id": "pvEw7y98xjHV"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the CSV file into a DataFrame\n",
        "tweets_df = pd.read_csv('/content/tweets_data_ingested.csv')\n",
        "\n",
        "# Now you can safely use the .head() method to view the first few rows\n",
        "print(tweets_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0gZo0TWymvc",
        "outputId": "72aa32ac-c15d-4c0d-ff0a-91658e748b90"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
            "0  567778009013178368          negative                        1.0000   \n",
            "1  569887533267611648          negative                        0.8563   \n",
            "\n",
            "     negativereason  negativereason_confidence     airline  \\\n",
            "0  Cancelled Flight                     1.0000      United   \n",
            "1       Late Flight                     0.5938  US Airways   \n",
            "\n",
            "  airline_sentiment_gold             name negativereason_gold  retweet_count  \\\n",
            "0               negative    realmikesmith    Cancelled Flight              0   \n",
            "1               negative  ConstanceSCHERE         Late Flight              0   \n",
            "\n",
            "                                                text  \\\n",
            "0  @united So what do you offer now that my fligh...   \n",
            "1  @USAirways Seriously doubt that as I am still ...   \n",
            "\n",
            "                   tweet_coord              tweet_created tweet_location  \\\n",
            "0  [26.37852293, -81.78472152]  2015-02-17 12:10:00 -0800        Chicago   \n",
            "1   [39.8805621, -75.23893393]  2015-02-23 07:52:30 -0800     Boston, MA   \n",
            "\n",
            "                user_timezone  \n",
            "0  Eastern Time (US & Canada)  \n",
            "1      Atlantic Time (Canada)  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Deliverables:\n",
        "# Python script for data pipeline implementation"
      ],
      "metadata": {
        "id": "UFr2_XenzeuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Read the Dataset\n",
        "try:\n",
        "    tweets_df = pd.read_csv(\"/content/tweets_data_ingested.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The specified file path is invalid. Please ensure the file exists.\")\n",
        "    exit()"
      ],
      "metadata": {
        "id": "YfuiJ8x2zhwU"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Handle Missing Values\n",
        "if tweets_df.isnull().values.any():\n",
        "    print(\"Warning: Missing values found in the dataset. Cleaning missing values...\")\n",
        "    tweets_df.dropna(inplace=True)\n",
        "    print(\"Missing values cleaned successfully.\")"
      ],
      "metadata": {
        "id": "q86Oz-y80YGw"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Remove Duplicates\n",
        "if tweets_df.duplicated().any():\n",
        "    print(\"Warning: Duplicate records found in the dataset. Removing duplicates...\")\n",
        "    tweets_df.drop_duplicates(inplace=True)\n",
        "    print(\"Duplicates removed successfully.\")"
      ],
      "metadata": {
        "id": "jKIcw9550dHZ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# : Duplicate Detection and Removal with Automated Data Quality Management\n",
        "\n",
        "# Automated Anomaly Detection Function\n",
        "def detect_anomalies(df):\n",
        "    # Implement anomaly detection techniques here\n",
        "    # For example, you can use statistical methods like z-score or IQR\n",
        "    # Return True if anomalies are detected, False otherwise\n",
        "    pass\n",
        "\n",
        "# Automated Missing Data Handling Function\n",
        "def handle_missing_data(df):\n",
        "    # Implement missing data handling techniques here\n",
        "    # For example, you can use imputation methods like mean, median, or mode\n",
        "    # Return True if missing data is handled, False otherwise\n",
        "    pass\n",
        "\n",
        "# Automated Duplicate Data Management Function\n",
        "def manage_duplicates(df):\n",
        "    # Implement duplicate data management techniques here\n",
        "    # For example, you can use pandas' drop_duplicates() function\n",
        "    # Return True if duplicates are managed, False otherwise\n",
        "    pass\n",
        "\n",
        "# Automated Data Quality Management Script\n",
        "def automated_data_quality_management(df):\n",
        "    # Automated Anomaly Detection\n",
        "    if detect_anomalies(df):\n",
        "        print(\"Anomalies detected and flagged.\")\n",
        "\n",
        "    # Automated Missing Data Handling\n",
        "    if handle_missing_data(df):\n",
        "        print(\"Missing data handled.\")\n",
        "\n",
        "    # Automated Duplicate Data Management\n",
        "    if manage_duplicates(df):\n",
        "        print(\"Duplicates managed.\")\n",
        "\n",
        "# Error Handling Mechanisms with Automated Data Quality Management\n",
        "try:\n",
        "    tweets_df = pd.read_csv(\"/content/tweets_data_ingested.csv\")\n",
        "    automated_data_quality_management(tweets_df)\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The specified file path is invalid. Please ensure the file exists.\")\n",
        "    exit()"
      ],
      "metadata": {
        "id": "KY9xhG8H2t6u"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Save the Processed Data\n",
        "try:\n",
        "    tweets_df.to_csv(\"cleaned_tweets_data.csv\", index=False)\n",
        "    print(\"Processed data saved successfully as 'cleaned_tweets_data.csv'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: An error occurred while saving the processed data - {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnJcsfDb0hjG",
        "outputId": "f365f20b-4675-4890-b319-2fa5d000da79"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed data saved successfully as 'cleaned_tweets_data.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Conclusion\n",
        "print(\"Data pipeline execution completed successfully. The processed data is ready for analysis.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Vu3rRLn0oD3",
        "outputId": "423988e6-6cc0-4793-c980-6532a08c27fd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data pipeline execution completed successfully. The processed data is ready for analysis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Deliverables:\n",
        "#Documentation explaining the pipeline design, including error handling mechanisms and data quality considerations\n"
      ],
      "metadata": {
        "id": "IhQQTO6-1TXr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}